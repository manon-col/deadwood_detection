{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0cb1bf6",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\\\n",
    "The raw data are point clouds generated by MLS (mobile LiDAR). The very first preprocessing step involves processing by the SLAM algorithm (more specifically, via the application provided by GeoSLAM), and unzip files.\n",
    "\n",
    "Then, the next part consists in classifying ground points, generating a DTM, and extracting vegetation points in a slice parallel to DTM. This step is carried out using the opensource software Computree, and is detailed in the file computree_steps.xsct2 (it is ready to use, only the input files need to be selected and the output folder specified, parameters can also be viewed and modified).\n",
    "\n",
    "Note: depending on the device you are using, it may be necessary to tile the point cloud before processing with Computree. In this case, make sure you convert to point format 7 and merge the output tile files before moving on. I recommend using CloudCompare for tiling and for converting .laz to .las.\n",
    "\n",
    "*Output folder: 'computree_outputs'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f008ba2",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\\\n",
    "*Input folder: 'computree_outputs'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages and modules\n",
    "import os\n",
    "import glob\n",
    "from clustering import ClEngine, Cluster\n",
    "\n",
    "las_files_path = 'computree_outputs' # location of preprocessed files\n",
    "clusters_las_path = 'clusters_las' # location of clustered files\n",
    "clusters_img_path = 'clusters_img'\n",
    "\n",
    "spheres_file = 'preprocessing/spheres_coordinates.csv' # file containing plot centre coordinates\n",
    "\n",
    "figsize = (4,4)\n",
    "dpi = 75\n",
    "\n",
    "# Listing all .las files to cluster\n",
    "las_files = glob.glob('computree_outputs/*.las')\n",
    "\n",
    "# Listing already clustered files\n",
    "clustered = glob.glob(clusters_las_path + '/*.las')\n",
    "clustered_names = [os.path.splitext(os.path.basename(file))[0] for file in clustered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e863ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in las_files:\n",
    "    \n",
    "    # Checking if the file is not already clustered\n",
    "    if os.path.splitext(os.path.basename(file))[0]+'_clusters' not in clustered_names:\n",
    "        \n",
    "        cl = ClEngine(file)\n",
    "        \n",
    "        # Cluster .las file points\n",
    "        cl.DBSCAN_clustering(eps=0.05, min_samples=100) # distance parameters are in meters\n",
    "        \n",
    "        # Filter clusters given on a minimum number of points and minimum length. Note that filter based on\n",
    "        # a minimum length take a long to process, set the min_dist param to \"None\" if you don't want to\n",
    "        # use it.\n",
    "        pla.filtering(nb_points=500,\n",
    "                      coord_file=spheres_file,\n",
    "                      sep=';',\n",
    "                      dec=',',\n",
    "                      distance_from_centre=18,\n",
    "                      delta=0.05,\n",
    "                      min_dist=None)\n",
    "        \n",
    "        # Draw a (very) basic representation of the clusters\n",
    "        # cl.draw_clusters()\n",
    "        \n",
    "        # Save clustering results in new .las files\n",
    "        cl.save_clusters_las(folder=clusters_las_path)\n",
    "        \n",
    "        # Save clustering results in .png files\n",
    "        cl.save_clusters_img(folder=clusters_img_path, figsize=figsize, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f0b39",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "\n",
    "## Image creation\n",
    "\\\n",
    "First, we need to create images from the previously generated points clouds of cylindrical shapes. Graphs are generated from the points by colouring them according to their z coordinate, and are exported in .png format.\n",
    "\n",
    "*Input folder: 'shapes_raw'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89724c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages and modules\n",
    "import os\n",
    "import glob\n",
    "import image_creation as imgcreate\n",
    "\n",
    "# Path of folder with folders containing the files from which to create images\n",
    "path_raw = 'shapes_raw'\n",
    "img_path = 'shapes_img'\n",
    "\n",
    "# Image sizing\n",
    "image_size_inches = (4, 4) # in inches\n",
    "dpi = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccff43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse all files and create images from each file\n",
    "for folder in glob.glob(path_raw+'/*'):\n",
    "    \n",
    "    folder_name = os.path.splitext(os.path.basename(folder))[0]\n",
    "    dest = img_path + '/' + folder_name\n",
    "    \n",
    "    imgcreate.image_generator(data_folder=folder, img_folder=dest, figsize=image_size_inches, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6f344",
   "metadata": {},
   "source": [
    "*Output folder: 'shapes_img'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297be46",
   "metadata": {},
   "source": [
    "## NNCLR model building\n",
    "\\\n",
    "The aim is not to spend our time manually classifying images, so we will be using a NNCLR model, adapted to a small labelled training set. The NNCLR model used here is based on the example of https://keras.io/examples/vision/nnclr/ (see link for more details).\n",
    "\n",
    "*Input folder: 'NNCLR_data', containing images for training*\n",
    "\n",
    "```\n",
    "NNCLR_data\n",
    "│\n",
    "├── labelled\n",
    "│   ├── deadwood\n",
    "│   │   ├── image1.png\n",
    "│   │   ├── image2.png\n",
    "│   │   ├── ...\n",
    "│   │\n",
    "│   └── other\n",
    "│       ├── image1.png\n",
    "│       ├── image2.png\n",
    "│       ├── ...\n",
    "│\n",
    "└── unlabelled\n",
    "    └── unlabelled\n",
    "        ├── image1.png\n",
    "        ├── image2.png\n",
    "        ├── ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3cb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import image_classification as imgclf\n",
    "\n",
    "device = 'CPU:0' # device used to run tensorflow\n",
    "model_path = 'NNCLR_data' # path with labelled and unlabelled images for training\n",
    "save_path = model_path + '/finetuning_model'\n",
    "batch_size = 32\n",
    "num_epochs = 50 # max number of epochs, the model will stop automatically when val_p_loss has not increased for 5 epochs\n",
    "image_size = (300, 300) # in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7701e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    \n",
    "    # Initialise model hyperparameters. For information:\n",
    "    # input_shape = (image_size[0], image_size[1], 3)\n",
    "    # AUTOTUNE = tf.data.AUTOTUNE\n",
    "    # shuffle_buffer = 5000   \n",
    "    # temperature = 0.1\n",
    "    # queue_size = 10000\n",
    "    # contrastive_augmenter = {\n",
    "    #     \"brightness\": 0.5,\n",
    "    #     \"name\": \"contrastive_augmenter\",\n",
    "    #     \"scale\": (0.2, 1.0)}\n",
    "    # classification_augmenter = {\n",
    "    #     \"brightness\": 0.2,\n",
    "    #     \"name\": \"classification_augmenter\",\n",
    "    #     \"scale\": (0.5, 1.0)}\n",
    "    # width = 128\n",
    "    model = imgclf.Model(model_path, image_size, batch_size, num_epochs)\n",
    "    \n",
    "    # Prepare training and validation datasets\n",
    "    model.prepare_dataset()\n",
    "    \n",
    "    # Pre-train NNCLR\n",
    "    model.pretraining()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.finetuning(save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f869e",
   "metadata": {},
   "source": [
    "## Identification of deadwood images and conversion into .las files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import laspy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shape_processing as sp\n",
    "import image_classification as imgclf\n",
    "\n",
    "# Needed paths\n",
    "model_path = 'NNCLR_data'\n",
    "save_path = model_path + '/finetuning_model'\n",
    "path_raw = 'shapes_raw'\n",
    "path_img = 'shapes_img'\n",
    "path_dw = 'deadwood'\n",
    "spheres_file = 'preprocessing/spheres_coordinates.csv'\n",
    "\n",
    "# Same as before\n",
    "device = 'CPU:0'\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "image_size = (300, 300)\n",
    "\n",
    "# Get already classified file names\n",
    "classified_files = glob.glob(path_dw+'/*.las')\n",
    "classified_names = [os.path.splitext(os.path.basename(file))[0] for file in classified_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create model\n",
    "model = imgclf.Model(model_path, image_size, batch_size, num_epochs)\n",
    "model.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse folder for each study area\n",
    "for folder in glob.glob(path_img+'/*'):\n",
    "    \n",
    "    folder_name = os.path.splitext(os.path.basename(folder))[0]\n",
    "    \n",
    "    #Check if classification is not already done\n",
    "    if folder_name + '_deadwood' not in classified_names:\n",
    "        \n",
    "        print(\"Classifying \" + folder_name + \" shapes.\")\n",
    "        \n",
    "        # Initialise counters...\n",
    "        total = 0 # of total number of shapes\n",
    "        dw = 0 # of identified deadwood shapes\n",
    "        \n",
    "        # Initialise new las file\n",
    "        path_out = path_dw+'/'+folder_name+ '_deadwood.las'\n",
    "        new_las = laspy.create(point_format=7, file_version=\"1.4\")\n",
    "        new_las.header.scales = np.array([1.e-05, 1.e-05, 1.e-05])\n",
    "        new_las.write(path_out)\n",
    "        \n",
    "        # Browse all shapes of the study area\n",
    "        for image in glob.glob(folder+'/*.png'):\n",
    "            \n",
    "            # Make a prediction with the model: each image classified as deadwood by the model, and\n",
    "            # classified as \"other\" but with a score < treshold are kept in the final point cloud\n",
    "            if model.prediction(image, treshold=0.8):\n",
    "                \n",
    "                image_name = os.path.splitext(os.path.basename(image))[0]\n",
    "                shape_file = path_raw + '/' + folder_name + '/' + image_name + '.txt'\n",
    "                shape = sp.shape_processing(shape_file)\n",
    "                \n",
    "                # Additional filters\n",
    "                \n",
    "                # Filtering by inclination to the vertical\n",
    "                shape.inclination_filter(angle=40)\n",
    "                \n",
    "                # Filtering \"flying\" branches\n",
    "                shape.flying_filter()\n",
    "                \n",
    "                # Filtering shapes outside the inventory plot\n",
    "                shape.distance_from_centre(plot_name=folder_name, coord_file='preprocessing/spheres_coordinates.csv')\n",
    "                \n",
    "                if not shape.is_filtered():\n",
    "                    \n",
    "                    dw += 1\n",
    "                    \n",
    "                    # Create .las points\n",
    "                    points = shape.las_points(header=new_las.header, label=dw)\n",
    "\n",
    "                    # Append .las points to new file\n",
    "                    with laspy.open(path_out, mode=\"a\") as las_out:\n",
    "                        las_out.append_points(points)                \n",
    "            \n",
    "            total += 1\n",
    "        \n",
    "        print(str(dw)+\" shapes classified as deadwood out of \"+str(total)+\".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da810229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
