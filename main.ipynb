{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0cb1bf6",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\\\n",
    "The raw data are point clouds generated by MLS (mobile LiDAR). The very first pre-processing step involves processing by the SLAM algorithm (more specifically, via the application provided by GeoSLAM), and unzip files.\n",
    "\n",
    "Then, the next part consists in classifying ground points, generating a DTM, and extracting vegetation points in a slice parallel to DTM. This step is carried out using the opensource software Computree, and is detailed in the file computree_steps.xsct2 (it is ready to use, only the input files need to be selected and the output folder specified, parameters can also be viewed and modified).\n",
    "\n",
    "Note: depending on the device you are using, it may be necessary to tile the point cloud before processing with Computree. In this case, make sure you convert to point format 7 and merge the output tile files before moving on. I recommend using CloudCompare for tiling and for converting .laz to .las.\n",
    "\n",
    "*Output folder: 'computree_outputs'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f008ba2",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\\\n",
    "A shape (branch, trunk, etc.) is characterised by a high point density. To eliminate noise such as foliage, point clouds are clustered using a DBSCAN (density-based spatial clustering of applications with noise) algorithm. The resulting clusters are then clustered more finely using the HDBSCAN method, and clusters are filtered in order to eliminate high branches and shapes that are short in length.\n",
    "In case, other filters are available, based on a minimum distance from plot centre or a minimum number of cluster points.\n",
    "\n",
    "The clustering results are exported as a point cloud (.las file), and as individual cluster images for classification.\n",
    "\n",
    "*Input folder: 'computree_outputs'*  \n",
    "*Output folders: 'clusters_las' (.las files), 'clusters_img' (.png files)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages and modules\n",
    "import os\n",
    "import glob\n",
    "import laspy\n",
    "from clustering import ClEngine, Cluster\n",
    "\n",
    "las_files_path = 'computree_outputs' # location of preprocessed files\n",
    "clusters_las_path = 'clusters_las' # location of clustered .las files\n",
    "clusters_img_path = 'clusters_img' # location of cluster images\n",
    "\n",
    "# Cluster image params\n",
    "figsize = (4,4)\n",
    "dpi = 75\n",
    "\n",
    "# Listing all .las files to cluster\n",
    "las_files = glob.glob(las_files_path + '/*.las')\n",
    "\n",
    "# Listing already clustered files\n",
    "clustered = glob.glob(clusters_las_path + '/*.las')\n",
    "clustered_names = [os.path.splitext(os.path.basename(file))[0] for file in clustered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e863ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for file in las_files:\n",
    "    \n",
    "    # Checking if the file is not already clustered\n",
    "    if os.path.splitext(os.path.basename(file))[0]+'_cl' not in clustered_names:\n",
    "        \n",
    "        cl = ClEngine(file)\n",
    "        \n",
    "        # Cluster .las file points with DBSCAN\n",
    "        cl.DBSCAN_clustering(eps=0.04, min_samples=50)\n",
    "        cl.save_all_points(folder=clusters_las_path, suffix=\"dbscan\") # backup of DBSCAN clusters\n",
    "        \n",
    "        # Cluster points finely using HDBSCAN algorithm (when there is too much vegetation points)\n",
    "        cl.HDBSCAN_clustering(min_cluster_size=500, max_cluster_size=250000, min_samples=25)\n",
    "        cl.save_all_points(folder=clusters_las_path, suffix=\"hdbscan\") # backup of HDBSCAN clusters\n",
    "        \n",
    "        # Filter clusters given on a minimum number of points, distance from the plot centre, distance from the ground,\n",
    "        # and minimum length. Set argument to None to ignore filter.\n",
    "        cl.filtering(delta=0.75, min_dist=0.5)\n",
    "        \n",
    "        # Save final clustering results in a new .las file\n",
    "        cl.save_all_points(folder=clusters_las_path, suffix=\"cl\")\n",
    "        \n",
    "        # Save clustering results in .png files\n",
    "        cl.save_individual_images(folder=clusters_img_path, figsize=figsize, dpi=dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297be46",
   "metadata": {},
   "source": [
    "## NNCLR model building\n",
    "\\\n",
    "The aim is not to spend our time manually classifying images, so we will be using a NNCLR model, adapted to a small labelled training set. The NNCLR model used here is based on the example of https://keras.io/examples/vision/nnclr/ (see link for more details).\n",
    "\n",
    "A training dataset was created from manually classified images of 2 sample plots (07_04 and BM08), and unlabelled images\n",
    "from 4 sample plots (BM01, BM02, BM03, BM04). Labelled images were divided into 2 classes: \"deadwood\" and \"other\". The folder architecture for training is as follows:\n",
    "\n",
    "\n",
    "```\n",
    "NNCLR_data\n",
    "│\n",
    "├── labelled\n",
    "│   ├── deadwood\n",
    "│   │   ├── image1.png\n",
    "│   │   ├── image2.png\n",
    "│   │   ├── ...\n",
    "│   │\n",
    "│   └── other\n",
    "│       ├── image1.png\n",
    "│       ├── image2.png\n",
    "│       ├── ...\n",
    "│\n",
    "└── unlabelled\n",
    "    └── unlabelled\n",
    "        ├── image1.png\n",
    "        ├── image2.png\n",
    "        ├── ...\n",
    "```\n",
    "\n",
    "*Input/output folder: 'NNCLR_data', containing images for training*\n",
    "\n",
    "Model hyperparameters, for information:\n",
    "\n",
    "```\n",
    "input_shape = (300, 300, 3)\n",
    "shuffle_buffer = 5000   \n",
    "temperature = 0.1\n",
    "queue_size = 98304\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3cb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import image_classification as imgclf\n",
    "\n",
    "model_path = 'NNCLR_data' # path with labelled and unlabelled images for training\n",
    "device = 'CPU:0' # device used to run tensorflow\n",
    "batch_size = 32\n",
    "num_epochs = 200 # max number of epochs, will stop before because early stopping is implemented\n",
    "image_size = (300, 300) # in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7701e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU memory is insufficient, the CPU can be used (calculations will take longer)\n",
    "with tf.device(device):\n",
    "    \n",
    "    # Initialise the model\n",
    "    model = imgclf.Model(model_path=model_path, image_size=image_size, batch_size=batch_size,\n",
    "                         num_epochs=num_epochs, encoder='custom')\n",
    "    \n",
    "    # Prepare training and validation datasets\n",
    "    model.prepare_dataset()\n",
    "    \n",
    "    # Visualise augmentations\n",
    "    # model.visualize_augmented_images(model._labelled_train_ds, num_images=10)\n",
    "    \n",
    "    # Pre-train NNCLR\n",
    "    model.pretraining(patience=10) # keep weights such that val_p_loss is the lowest and has not increased in 10 epochs\n",
    "    \n",
    "    # Perform fine tuning, save finetuning model\n",
    "    model.finetuning(patience=20, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f869e",
   "metadata": {},
   "source": [
    "## Image classification\n",
    "\\\n",
    "*Input folders: 'NNCLR_data', 'clusters_las', 'clusters_img'*  \n",
    "*Output folder: 'deadwood'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import laspy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import image_classification as imgclf\n",
    "from clustering import ClEngine, Cluster, label_from\n",
    "\n",
    "# Needed paths\n",
    "model_path = 'NNCLR_data'\n",
    "path_las = 'clusters_las'\n",
    "path_img = 'clusters_img'\n",
    "path_dw = 'deadwood'\n",
    "\n",
    "device = 'CPU:0'\n",
    "\n",
    "# Get already classified file names\n",
    "classified_files = glob.glob(path_dw+'/*.las')\n",
    "classified_names = [os.path.splitext(os.path.basename(file))[0] for file in classified_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "\n",
    "    # Re-create the model\n",
    "    model = imgclf.Model(model_path=model_path, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "\n",
    "    # Browse of each study area folder\n",
    "    for folder in glob.glob(path_img+'/*'):\n",
    "\n",
    "        folder_name = os.path.splitext(os.path.basename(folder))[0]\n",
    "\n",
    "        # Check if classification is not already done\n",
    "        if folder_name + '_dw' not in classified_names:\n",
    "\n",
    "            las_file = path_las + '/' + folder_name + '_cl.las'\n",
    "\n",
    "            # Load clusters\n",
    "            cl = ClEngine(las_file)\n",
    "                        \n",
    "            # List of cluster images\n",
    "            images = glob.glob(folder+'/*.png')\n",
    "\n",
    "            # Make a prediction with the model: each image classified as deadwood by the model, and\n",
    "            # classified as \"other\" but with a score < threshold are kept in the final point cloud\n",
    "            deadwood_images = model.prediction(images)\n",
    "\n",
    "            # List of clusters (cluster labels) classified as deadwood\n",
    "            deadwood_labels = [label_from(image) for image in deadwood_images]\n",
    "\n",
    "            # Keep clusters classified as deadwood\n",
    "            cl.keep_clusters(cluster_list=deadwood_labels, relabel=False)\n",
    "            \n",
    "            # Save clusters classified as deadwood\n",
    "            cl.save_all_points(folder=path_dw, suffix='dw')\n",
    "            \n",
    "            # Calculate and save inventory volumes\n",
    "            cl.filtering(distance_from_centre=19)  # add tolerance\n",
    "            cl.calculate_volumes(alpha=50)\n",
    "            cl.save_volumes_csv(folder=path_dw, suffix='volumes_inventory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705727f9",
   "metadata": {},
   "source": [
    "In order to determine the percentage of deadwood that could be detectable before classification, each reference deadwood item was manually matched with clusters (see field_inventory.csv). Now, we can calculate the volume of these clusters, for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8734089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering import ClEngine, Cluster\n",
    "import pandas as pd\n",
    "\n",
    "inventory_file = 'field_inventory/field_inventory.csv'\n",
    "las_file_path = 'clusters_las'\n",
    "deadwood_path = 'deadwood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(inventory_file, sep=';', decimal=',')\n",
    "plots = df['plot'].unique()\n",
    "\n",
    "for plot in plots[5:6]:\n",
    "\n",
    "    filtered_df = df[df['plot'] == plot]\n",
    "    detectable_labels = []\n",
    "\n",
    "    # Browse each line of the dataframe\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        detectable_label_cell = row['id_cluster_detectable']\n",
    "\n",
    "        if pd.notna(detectable_label_cell): # ignore NaN values\n",
    "\n",
    "            if isinstance(detectable_label_cell, float):\n",
    "                detectable_labels.append(int(detectable_label_cell))\n",
    "\n",
    "            else:\n",
    "                values = detectable_label_cell.split(';') # get values separated by ';'\n",
    "\n",
    "                for value in values:\n",
    "                    if value.strip():  # ignore null values\n",
    "                        detectable_labels.append(int(value))\n",
    "\n",
    "    detectable_labels = list(set(detectable_labels)) # delete duplicates\n",
    "    \n",
    "    # Load the cluster .las file of the current plot\n",
    "    las_file = f\"{las_file_path}/{plot}_cl.las\"\n",
    "    cl = ClEngine(las_file)\n",
    "    \n",
    "    # Keep only detected clusters\n",
    "    cl.keep_clusters(detectable_labels, relabel=False)\n",
    "    \n",
    "    # Calculate and save volumes\n",
    "    cl.calculate_volumes(alpha=50, plot=True)\n",
    "    cl.save_volumes_csv(folder=deadwood_path, suffix='volumes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eba422",
   "metadata": {},
   "source": [
    "We can also determine which clusters were detectable but not detected. This makes it possible to calculate the actual performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a053d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot in plots:\n",
    "\n",
    "    filtered_df = df[df['plot'] == plot]\n",
    "    detectable_labels = []\n",
    "\n",
    "    # Browse each line of the dataframe\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        detectable_label_cell = row['id_cluster_detectable']\n",
    "\n",
    "        if pd.notna(detectable_label_cell): # ignore NaN values\n",
    "\n",
    "            if isinstance(detectable_label_cell, float):\n",
    "                detectable_labels.append(int(detectable_label_cell))\n",
    "\n",
    "            else:\n",
    "                values = detectable_label_cell.split(';') # get values separated by ';'\n",
    "\n",
    "                for value in values:\n",
    "                    if value.strip():  # ignore null values\n",
    "                        detectable_labels.append(int(value))\n",
    "\n",
    "    detectable_labels = list(set(detectable_labels)) # delete duplicates\n",
    "    \n",
    "    # Load the deadwood .las file of the current plot\n",
    "    las_file = f\"{deadwood}/{plot}_cl_dw.las\"\n",
    "    cl = ClEngine(las_file)\n",
    "    \n",
    "    # Get the list of detected clusters\n",
    "    detected_labels = cl.get_clusters()\n",
    "    \n",
    "    undetected = [cluster for cluster in detectable_labels if cluster not in detected_labels]\n",
    "\n",
    "    print(f\"Clusters of {plot} detectable but not detected: {undetected})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
