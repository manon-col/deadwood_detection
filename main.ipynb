{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0cb1bf6",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\\\n",
    "The raw data are point clouds generated by MLS (mobile LiDAR). The very first preprocessing step involves processing by the SLAM algorithm (more specifically, via the application provided by GeoSLAM), and unzip files.\n",
    "\n",
    "Then, the next part consists in classifying ground points, generating a DTM, and extracting vegetation points in a slice parallel to DTM. This step is carried out using the opensource software Computree, and is detailed in the file computree_steps.xsct2 (it is ready to use, only the input files need to be selected and the output folder specified, parameters can also be viewed and modified).\n",
    "\n",
    "Note: depending on the device you are using, it may be necessary to tile the point cloud before processing with Computree. In this case, make sure you convert to point format 7 and merge the output tile files before moving on. I recommend using CloudCompare for tiling and for converting .laz to .las.\n",
    "\n",
    "*Output folder: 'computree_outputs'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f008ba2",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\\\n",
    "*Input folder: 'computree_outputs'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77e5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading packages and modules\n",
    "import os\n",
    "import glob\n",
    "from clustering import ClEngine, Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21056ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 07_04_ct.las loaded successfully.\n",
      "Clustering 07_04_ct.las points...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file))[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_clusters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \\\n\u001b[0;32m     15\u001b[0m clustered_names:\n\u001b[0;32m     17\u001b[0m     cl \u001b[38;5;241m=\u001b[39m ClEngine(file)\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mcl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDBSCAN_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     cl\u001b[38;5;241m.\u001b[39mfiltering()\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# cl.draw_clusters()\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Work\\deadwood_detection\\clustering.py:74\u001b[0m, in \u001b[0;36mDBSCAN_clustering\u001b[1;34m(self, eps, min_samples)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mDBSCAN_clustering\u001b[39m(\u001b[38;5;28mself\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    An euclidian clustering operation of points of the las file, using the\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    DBSCAN algorithm.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    eps : float, optional\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        The maximum distance between two samples for one to be considered\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m        as in the neighborhood of the other. This is not a maximum bound on\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m        the distances of points within a cluster. This is the most\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m        important DBSCAN parameter to choose appropriately for your data\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;124;03m        set and distance function. The default is 0.05m.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    min_samples : float, optional\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m        The number of samples (or total weight) in a neighborhood for a\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m        point to be considered as a core point. This includes the point\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m        itself. The default is 100.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClustering \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.las points...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# Running DBSCAN algorithm\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deadwood\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deadwood\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py:412\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    409\u001b[0m core_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_neighbors \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    410\u001b[0m dbscan_inner(core_samples, neighborhoods, labels)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_sample_indices_ \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore_samples\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_ \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_sample_indices_):\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# fix for scipy sparse indexing issue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "las_files_path = 'computree_outputs' # location of preprocessed files\n",
    "clustered_files_path = 'cluster_outputs' # location of clustered files\n",
    "\n",
    "# Listing all .las files to cluster\n",
    "las_files = glob.glob('computree_outputs/*.las')\n",
    "\n",
    "# Listing clustered files\n",
    "clustered = glob.glob('cluster_outputs/*.las')\n",
    "clustered_names = [os.path.splitext(os.path.basename(file))[0] for file in clustered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e863ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in las_files:\n",
    "    \n",
    "    # Checking if the file is not already clustered\n",
    "    if os.path.splitext(os.path.basename(file))[0]+'_clusters' not in clustered_names:\n",
    "        \n",
    "        cl = ClEngine(file)\n",
    "        \n",
    "        # Cluster .las file points\n",
    "        cl.DBSCAN_clustering(eps=0.05, min_samples=100) # distance parameters are in meters\n",
    "        \n",
    "        # Filter clusters given on a minimum number of points and minimum length\n",
    "        cl.filtering(nb_points=500, min_dist=1)\n",
    "        \n",
    "        # Draw a (very) basic representation of the clusters\n",
    "        cl.draw_clusters()\n",
    "        \n",
    "        # Save clustering results in new .las files (filename_clusters.las)\n",
    "        cl.save_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a975c2",
   "metadata": {},
   "source": [
    "# Cylinder fitting\n",
    "\\\n",
    "A RANSAC cylinder fitting operation must be carried out. Here, it is carried out in CloudCompare, with the following parameters:\n",
    "    -...\n",
    "\n",
    "This step is also a visual validation step.\n",
    "\n",
    "The point clouds of detected cylindrical shapes (not mesh !) must be exported in cloud ascii format (.txt), with headers as column titles. As there will be mutliple files, make sure you save them in a sub-folder.\n",
    "\n",
    "*Output folder: 'shapes_raw/plot_name_or_reference_or_what_you_want'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f0b39",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "\n",
    "First, we need to create images from the previously generated points clouds of cylindrical shapes. The images are created from graphs in which the colour of the points depends on the z coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89724c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading modules\n",
    "import image_creation as imgcreate\n",
    "import image_classification as imgclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df945221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of folder with folders containing the files from which to create images\n",
    "path_raw = 'shapes_raw'\n",
    "\n",
    "# Path of folder containing image folders\n",
    "dataset_path = 'CNN_data'\n",
    "\n",
    "# Size of images\n",
    "image_size_inches = (4, 4) # in inches\n",
    "image_size = (217, 217) # in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccff43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse all files and create images from each file\n",
    "for folder in glob.glob(path_raw+'/*'):\n",
    "    \n",
    "    folder_name = os.path.splitext(os.path.basename(folder))[0]\n",
    "    dest = dataset_path + '/' + folder_name\n",
    "    \n",
    "    imgcreate.image_generator(data_folder=folder, img_folder=dest, figsize=image_size_inches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297be46",
   "metadata": {},
   "source": [
    "Now that the images are created, we can train an image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'CNN_data/07_04' # location of image data to train the model\n",
    "\n",
    "train_ds, val_ds = imgclf.dataset_generation(folder)\n",
    "train_ds_augmented = imgclf.augmentation(train_ds)\n",
    "\n",
    "model = imgclf.make_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
